<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Pen Detector AI</title>
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=no" />
  <style>
    body {
      font-family: Arial, sans-serif;
      background: #f4f4f4;
      text-align: center;
      margin: 0;
      padding: 20px;
    }
    video {
      border: 2px solid #333;
      border-radius: 12px;
      width: 90vw;
      max-width: 480px;
      height: auto;
      margin-top: 20px;
    }
    #result {
      margin-top: 20px;
      font-size: 1.5em;
      font-weight: bold;
      color: #333;
    }
    #switchCam {
      margin-top: 15px;
      padding: 10px 20px;
      font-size: 1em;
      border: none;
      border-radius: 8px;
      background-color: #007BFF;
      color: white;
      cursor: pointer;
    }
    #switchCam:active {
      background-color: #0056b3;
    }
  </style>
</head>
<body>
  <h1>Pen Recognition</h1>
  <video id="webcam" autoplay playsinline muted></video>
  <br>
  <button id="switchCam">Switch Camera</button>
  <div id="result">Loading model...</div>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@4.10.0"></script>

  <script>
    const video = document.getElementById("webcam");
    const resultDiv = document.getElementById("result");
    const switchCamButton = document.getElementById("switchCam");

    const classNames = ["Pen", "Nothing"];
    const modelURL = "model/model.json";
    const inputSize = 96;
    let model;
    let usingBackCamera = true;
    let stream;
    let stopPrediction = false;

    async function getCameraStream() {
      if (stream) {
        stream.getTracks().forEach(track => track.stop());
      }

      const constraints = {
        video: {
          facingMode: usingBackCamera ? { exact: "environment" } : "user"
        }
      };

      try {
        stream = await navigator.mediaDevices.getUserMedia(constraints);
        video.srcObject = stream;

        return new Promise(resolve => {
          video.onloadedmetadata = () => resolve();
        });
      } catch (err) {
        resultDiv.innerText = `Camera error: ${err.message}`;
        throw err;
      }
    }

    async function loadModel() {
      model = await tf.loadLayersModel(modelURL);
      resultDiv.innerText = "Model loaded!";
    }

    async function predictLoop() {
      stopPrediction = false;

      while (!stopPrediction) {
        tf.engine().startScope();

        if (video.readyState === 4 && model) {
          const tensor = tf.browser.fromPixels(video)
            .resizeBilinear([inputSize, inputSize])
            .toFloat()
            .div(255)
            .expandDims(0);

          const prediction = await model.predict(tensor).data();
          displayPrediction(Array.from(prediction));
        }

        tf.engine().endScope();
        await new Promise(resolve => setTimeout(resolve, 300)); // 300ms delay
      }
    }

    function displayPrediction(predictions) {
      const maxIndex = predictions.indexOf(Math.max(...predictions));
      const label = classNames[maxIndex];
      const confidence = (predictions[maxIndex] * 100).toFixed(1);
      resultDiv.innerText = `Prediction: ${label} (${confidence}%)`;
    }

    switchCamButton.addEventListener("click", async () => {
      stopPrediction = true;
      resultDiv.innerText = `Switching to ${usingBackCamera ? "Front" : "Back"} Camera...`;

      usingBackCamera = !usingBackCamera;

      await getCameraStream();
      predictLoop(); // Restart prediction loop
    });

    (async () => {
      await getCameraStream();
      await loadModel();
      predictLoop();
    })();
  </script>
</body>
</html>
